#+BEGIN_COMMENT
.. title: Training A Non-Neural Net Model
.. slug: training-a-non-neural-net-model
.. date: 2018-06-01 13:23:09 UTC-07:00
.. tags: training tensorflow
.. category: training
.. link: 
.. description: Tranining simple models
.. type: text
#+END_COMMENT

* Imports
  This is the tensorflow style guide recommendation to make the code backwards-compatible with python 2.7. I don't think it makes sense anymore, since python 2 is nearing the for-real-this-time end-of-life date (sometime in 2010). Oh, well.
#+BEGIN_SRC ipython :session training :results none
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function
#+END_SRC

#+BEGIN_SRC ipython :session training :results none
# python standard library
import os

# from pypi
import tensorflow as tf

import matplotlib.pyplot as pyplot
import numpy
import seaborn
#+END_SRC

#+BEGIN_SRC ipython :session training :results none
% matplotlib inline
seaborn.set()
seaborn.set_style("whitegrid", {"axes.grid": False})
#+END_SRC

* A Polynomial Loss Model
  We're going to build a model using a polynomial loss function. Here's what it looks like.
  
#+BEGIN_SRC ipython :session training :results none
x = numpy.linspace(-5, 5)
y = 2*x**2 + 3*x + 4
#+END_SRC

#+BEGIN_SRC ipython :session training :results raw drawer :ipyfile /tmp/polynomial_loss.png
figure = pyplot.figure()
axe = figure.gca()
axe.set_title("Loss Function")
plot = pyplot.plot(x, y)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[29]:
[[file:/tmp/polynomial_loss.png]]
:END:

** Setup
   The first thing we will do is setup some variables for the training. The [[https://www.tensorflow.org/programmers_guide/variables][=tensorflow.Variable=]] is a tensor (but not a [[https://www.tensorflow.org/programmers_guide/tensors][=tensorflow.Tensor=]]) whose values can be updated. It maintains its value in between [[https://www.tensorflow.org/api_docs/python/tf/Session][Sessions]] so if you run multiple sessions the value wil carry over to each run. Its constructor takes these arguments.

| Argument         | Description                                                          | Default                  |
|------------------+----------------------------------------------------------------------+--------------------------|
| =initial_value=  | Tensor that represents the starting value                            | None                     |
| =trainable=      | If True, added to the collection of variables used by the Optimizers | True                     |
| =collections=    | List of graph collection keys that the variable gets added to        | None                     |
| =validate_shape= | If True, the shape of =initial_value= must be known                  | True                     |
| =caching_device= | Device string describing where to cache the Variable                 | None                     |
| =name=           | Name for the variable                                                | Variable + unique string |
| =variable_def=   | Protocol buffer to use to recreate a variable                        | None                     |
| =dtype=          | If set, the =initial_value= will be converted to it                  | None                     |
| =expected_shape= | If set, =initial_value= should have this shape                       | None                     |
| =import_scope=   | Name scope to add if initializing from a protocol buffer             | None                     |
| =constraint=     | Optional function to apply after being updated by an Optimizer       | None                     |

First we'll make a trainable variable with an initial value of 0.

#+BEGIN_SRC ipython :session training :results none
# Define a trainable variable
x_var = tf.Variable(0., name='XTrainable')
#+END_SRC

#+BEGIN_SRC ipython :session training :results none
# Define an untrainable variable to hold the global step
step_var = tf.Variable(0, trainable=False)

# Express loss in terms of the variable
loss = x_var * x_var - 4.0 * x_var + 5.0

# Find variable value that minimizes loss
learn_rate = 0.1
num_epochs = 40
optimizer = tf.train.GradientDescentOptimizer(learn_rate).minimize(loss, global_step=step_var)

# Initialize variables
init = tf.global_variables_initializer()

# Create the saver
saver = tf.train.Saver()

# Create summary data and FileWriter
summary_op = tf.summary.scalar('x', x_var)
file_writer = tf.summary.FileWriter('log', graph=tf.get_default_graph())

# Launch session
with tf.Session() as sess:
    sess.run(init)

    for epoch in range(num_epochs):
        _, step, result, summary = sess.run([optimizer, step_var, x_var, summary_op])
        print('Step %d: Computed result = %f' % (step, result))

        # Print summary data
        file_writer.add_summary(summary, global_step=step)
        file_writer.flush()

    # Store variable data
    saver.save(sess, os.getcwd() + '/output')
    print('Final x_var: %f' % sess.run(x_var))

#+END_SRC
